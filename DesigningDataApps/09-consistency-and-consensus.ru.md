# 9. Consistency and Consensus
[english](09-consistency-and-consensus.md) | russian

**TL;DR:** Модели консистентности, линеаризуемость и причинность; тотальный порядок и консенсус; почему 2PC и координация дорогие и где они реально нужны.

---

## Consistency Guarantees

- Консистентность — это обещания про видимость и порядок операций: какие чтения допустимы после каких записей.
- Нужно выбирать гарантии под задачу: causal часто достаточно для UX, linearizability нужна для замков/уникальности.

## Linearizability

- Линеаризуемость = система ведет себя как единственная копия: операции выглядят атомарными и упорядоченными по реальному времени.
- Если write завершился до read, то read обязан увидеть результат (для данного объекта/ключа).

### What Makes a System Linearizable?

- У каждой операции есть интервал выполнения; существует «точка линеаризации» внутри интервала, где операция как бы произошла мгновенно.
- Это сильнее, чем «eventual»: важна согласованность наблюдаемого порядка при конкуренции и задержках.

### Relying on Linearizability

- Когда нужна: распределенные блокировки и leader election, уникальные ограничения, зависимости между каналами (БД ↔ очередь).
- Без нее часто требуется дополнительная защита (например, fencing tokens) или перенос инварианта в один координирующий компонент.

### Implementing Linearizable Systems

- Обычно требуется координация: строгий лидер с подтверждением большинством или полноценный консенсус.
- Кворумы W+R>N сами по себе не гарантируют линейризуемость при concurrent writes и сетевых задержках.

### The Cost of Linearizability

- Цена: latency (ждем подтверждений) и потеря availability при network partition (CAP).
- В multi-DC стоимость растет из-за RTT; иногда выбирают causal/eventual ради доступности.

## Ordering Guarantees

- Для корректности важен порядок событий: причинность (happens-before), последовательности, тотальный порядок.
- Причинный порядок — частичный: многие события конкурентны и не сравнимы.

### Ordering and Causality

- Causal consistency гарантирует, что зависимые события видны в правильном порядке, но не навязывает порядок всем конкурентным событиям.
- Линеаризуемость сильнее causal: добавляет требование реального времени.

### Capturing causal dependencies

- Чтобы хранить причинность, нужны метаданные: version vectors, causal context, логические часы.
- Один скалярный timestamp не различает конкуренцию и причинность.

### Sequence Number Ordering

- Номера последовательности делают поток воспроизводимым, но генерация глобальной последовательности в распределении обычно ведет к консенсусу.
- Lamport timestamps сохраняют happens-before (если A→B, то ts(A)<ts(B)), но обратное неверно.

### Total Order Broadcast

- TOB: все узлы доставляют сообщения в одинаковом порядке; фундамент для state machine replication.
- Обычно реализуется через Raft/Paxos-класс (консенсус), или сводится к сильной координации.

## Distributed Transactions and Consensus

- 2PC дает атомарный коммит на нескольких узлах, но может блокировать участников при сбое координатора.
- Консенсус решает выбор лидера/порядка команд при отказах; это основа координационных сервисов.

### Atomic Commit and Two-Phase Commit (2PC)

- Фаза prepare: участники обещают возможность коммита (журнал + блокировки). Фаза commit/abort: координатор объявляет решение.
- Слабое место: «in doubt» — при падении координатора участники держат блокировки, пока не восстановят решение.

### Three-phase commit

- 3PC пытается убрать блокировку, но требует синхронных предположений (bounded delays), редко применимых в реальных сетях.

### Distributed Transactions in Practice

- Практические проблемы: удержание блокировок, сложное восстановление, высокая латентность, хрупкие режимы отказов.
- Частые альтернативы: саги, outbox/inbox, идемпотентность, derived state через стримы (event-driven).

## Fault-Tolerant Consensus

- Консенсус связан с TOB: единый лог команд = тотальный порядок.
- Epoch/term нумерация + кворумы позволяют отсекать старых лидеров и избегать split brain.

## Membership and Coordination Services

- Membership Service: cluster membership, failure detection, события join/leave. Централизованный vs децентрализованный membership. Невозможно точно определить падение (FLP theorem). heartbeat, gossip protocol, таймауты. Gossip-based membership. 
- Coordination Service: leader election, roles, sync state, configs, distributed locks. Paxos, Raft.
- Сложность: изменение состава кластера (membership) и корректные таймауты в мире частичных отказов.

### Allocating work to nodes

- Централизованный координатор: MapReduce master, Kubernetes scheduler
- Distributed queue: Kafka consumer groups, RabbitMQ, SQS
- Consistent hashing. Идея: ключ → hash → узел. При добавлении ноды перераспределяется только часть данных: Cassandra, DynamoDB, Redis Cluster.

### Service Discovery

- Hardcoded, client-side discovery, server-side discovery


## Summary

- Сильные гарантии (linearizability/consensus) дают корректность для критичных инвариантов, но стоят latency и availability при partition.
- Более слабые гарантии (causal) часто масштабируются лучше и достаточно для многих продуктовых сценариев.

---

## Термины

- **Consistency model:** правила видимости и порядка операций
- **Linearizability:** как единая копия данных с порядком по реальному времени
- **Causal consistency:** видимость причинных зависимостей без тотального порядка
- **Happens-before:** отношение причинности между событиями
- **Lamport timestamp:** логическое время: сохраняет happens-before, но не различает конкуренцию
- **Total Order Broadcast:** все доставляют сообщения в одном и том же порядке
- **Consensus:** согласование значения/лидера при сбоях
- **Epoch/term:** номер поколения лидера для отсечения старых лидеров
- **CAP theorem:** при partition нельзя одновременно сильную согласованность и доступность
- **2PC:** двухфазный коммит распределенной транзакции
- **Prepare:** обещание участника: «смогу закоммитить позже»
- **Lease:** временное право владения (часто с fencing)
