# 8. The Trouble with Distributed Systems
[english](08-the-trouble-with-distributed-systems.md) | russian

**TL;DR:** Почему распределенность ломает интуицию: частичные отказы, непредсказуемая сеть, ненадежные часы и паузы процессов — и какие архитектурные примитивы помогают.

---

## Faults and Partial Failures

- В распределенных системах сбои часто частичные: ломается один узел/линк/зона, а остальное продолжает работать — из-за этого сложно отличить «сбой» от «задержки».
- Отказ компонента (fault) не обязан превращаться в отказ сервиса (failure), если есть избыточность, деградация и восстановление.
- Главная сложность: неопределенность — «не ответил» не доказывает смерть, а «ответил» не гарантирует актуальность.

## Cloud Computing and Supercomputing

- Облачная среда обычно более «шумная»: shared hardware, виртуализация, миграции, непредсказуемые паузы и вариативные задержки.
- В HPC часто больше контроля над сетью и планированием, но это другая стоимость и другие допущения.
- Вывод: алгоритмы должны выдерживать джиттер, паузы процесса и деградации, а не рассчитывать на идеальную синхронность.

## Unreliable Networks

- Сеть может терять/дублировать/переупорядочивать/задерживать пакеты; поэтому «сеть надежна» — опасное допущение.
- Ретраи и таймауты неизбежны, но они могут усиливать перегрузку (retry storms), если не управлять backoff и лимитами.

### Network Faults in Practice

- Типичны краткие деградации: микропотери пакетов, временные сетевые разделения, проблемы маршрутизации.
- Сбои часто коррелированы: один инцидент затрагивает множество узлов (общая сеть/стойка/зона).
- Важно проектировать «контуры защиты»: rate limits, circuit breakers, bounded queues, backpressure.

### Detecting Faults

- Детектирование сбоя обычно основано на таймаутах и heartbeat — это статистика, а не доказательство.
- Нужно избегать «флаппинга» (частого переключения статуса): гистерезис, адаптивные таймауты, подавление шумов.
- Observability помогает отличать «умер» от «завис/в GC/перегружен».

### Timeouts and Unbounded Delays

- В асинхронной модели задержки не имеют верхней границы, поэтому строгого детектора падений не существует.
- Таймауты выбирают по перцентилям и SLO; слишком короткие дают ложные failover, слишком длинные — медленное восстановление.
- Ретраи должны быть с экспоненциальным backoff и джиттером, иначе усугубляют перегрузку.

### Network congestion and queueing

- Очереди порождают tail latency: один перегруженный компонент может затормозить весь end-to-end запрос.
- Перегрузка бывает на сети, на CPU (шифрование/обработка), на диске; симптомы похожи, лечение разное.
- Практика: ограничение очередей, приоритезация, load shedding, разрыв цепочек ретраев.

### Synchronous Versus Asynchronous Networks

- Синхронная сеть предполагает верхнюю границу задержек; интернет/ДЦ-большинство систем ближе к асинхронной модели.
- Частично предсказуемую сеть можно купить (QoS/выделенные линии), но процессные паузы и «шум» все равно останутся.

### Can we not simply make network delays predictable?

- Сделать задержки полностью предсказуемыми сложно и дорого; архитектура должна выдерживать вариативность.
- Даже идеальная сеть не спасает от пауз процесса, GC, миграций VM, contention и перегрузки.

## Unreliable Clocks

- Часы расходятся (clock skew) и могут «прыгать» при синхронизации; по wall-clock нельзя строить строгий порядок.
- Многие баги в распределении — это ошибки использования времени (timestamps для уникальности/порядка).

### Monotonic Versus Time-of-Day Clocks

- Time-of-day часы показывают календарное время и могут идти назад/вперед; monotonic часы монотонны и годятся для интервалов.
- Для таймаутов/latency нужен monotonic clock; для пользовательских меток времени — time-of-day.

### Clock Synchronization and Accuracy

- Синхронизация (NTP/PTP) дает приближение, но точность ограничена сетью и стабильностью осцилляторов.
- Правильная модель: время = значение ± ошибка (confidence interval), а не «точная точка».

### Relying on Synchronized Clocks

- Порядок по timestamp ломается при skew; если нужен порядок — логические часы или консенсус/тотальный лог.
- Глобальные снимки по времени возможны лишь при сильных гарантиях синхронизации и аккуратных протоколах.

## Process Pauses

- Процесс может «замереть» из-за GC, page faults, CPU starvation, suspend VM — это выглядит как сетевой сбой.
- Паузы ломают heartbeats и провоцируют ложные failover; нужны настройки и архитектурная изоляция latency-critical компонентов.

## Knowledge, Truth, and Lies

- Узлы имеют неполное знание; «истина» часто определяется большинством (quorum/majority). Примеры алгоритмов: Raft, Paxos, ZAB.
- Лидерство и блокировки требуют защиты от «зомби-лидера» — здесь важны fencing tokens.

### The Truth Is Defined by the Majority

- Кворум помогает избежать split brain и согласовать решение при частичных отказах.
- Но кворум работает при корректной модели членства и разумных таймаутах.

### The leader and the lock

- Лидер часто держит право записи/координации (лог/замок). Старый лидер может «вернуться» и писать — это опасно.
- Нужны механизмы «отсечения» старого лидера (epoch/term + fencing).

### Fencing tokens

- Fencing token — монотонно растущий номер владения; ресурс принимает команды только с наибольшим токеном.
- Защищает от stale leader даже при паузах и разделениях сети.

### Byzantine Faults

- Византийские сбои — произвольное/враждебное поведение. В обычных ДЦ чаще предполагают crash/omission faults.
- Тем не менее «мягкое вранье» бывает: битые данные, баги, неверные конфиги — нужны проверки целостности.

## System Model and Reality

- Формальные модели нужны, чтобы доказать свойства; реальный мир нарушает допущения (асинхронность, паузы, корреляции).
- Корректность делят на safety (никогда плохо) и liveness (когда-нибудь хорошо); часто жертвуют liveness ради safety.

## Summary

- Сложность распределения проистекает из частичных отказов, непредсказуемых задержек, ненадежных часов и пауз процессов.
- Лечится: таймауты+backoff, кворумы, fencing tokens, логические порядки и четкие критерии safety/liveness.
